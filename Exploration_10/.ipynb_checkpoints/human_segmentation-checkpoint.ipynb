{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "conditional-proof",
   "metadata": {},
   "source": [
    "# (1) # my_Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-teddy",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "together-morning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3024, 4032, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "import tarfile \n",
    "import os\n",
    "img_path = os.getenv('HOME')+'/aiffel/human_segmentation/images/my_image.jpg'  # 본인이 선택한 이미지의 경로에 맞게 바꿔 주세요. \n",
    "img_orig = cv2.imread(img_path) \n",
    "print (img_orig.shape)\n",
    "import urllib\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "looking-welding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3024, 4032, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "img_path = os.getenv('HOME')+'/aiffel/human_segmentation/images/my_image.jpg'  # 본인이 선택한 이미지의 경로에 맞게 바꿔 주세요. \n",
    "img_orig = cv2.imread(img_path) \n",
    "print (img_orig.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-matthew",
   "metadata": {},
   "source": [
    "## 2. DeepLab model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sophisticated-construction",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabModel(object):\n",
    "    INPUT_TENSOR_NAME = 'ImageTensor:0'\n",
    "    OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'\n",
    "    INPUT_SIZE = 513\n",
    "    FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
    "\n",
    "    # __init__()에서 모델 구조를 직접 구현하는 대신, tar file에서 읽어들인 그래프구조 graph_def를 \n",
    "    # tf.compat.v1.import_graph_def를 통해 불러들여 활용하게 됩니다. \n",
    "    def __init__(self, tarball_path):\n",
    "        self.graph = tf.Graph()\n",
    "        graph_def = None\n",
    "        tar_file = tarfile.open(tarball_path)\n",
    "        for tar_info in tar_file.getmembers():\n",
    "            if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n",
    "                file_handle = tar_file.extractfile(tar_info)\n",
    "                graph_def = tf.compat.v1.GraphDef.FromString(file_handle.read())\n",
    "                break\n",
    "        tar_file.close()\n",
    "\n",
    "        with self.graph.as_default():\n",
    "            tf.compat.v1.import_graph_def(graph_def, name='')\n",
    "\n",
    "        self.sess = tf.compat.v1.Session(graph=self.graph)\n",
    "\n",
    "    # 이미지를 전처리하여 Tensorflow 입력으로 사용 가능한 shape의 Numpy Array로 변환합니다.\n",
    "    def preprocess(self, img_orig):\n",
    "        height, width = img_orig.shape[:2]\n",
    "        resize_ratio = 1.0 * self.INPUT_SIZE / max(width, height)\n",
    "        target_size = (int(resize_ratio * width), int(resize_ratio * height))\n",
    "        resized_image = cv2.resize(img_orig, target_size)\n",
    "        resized_rgb = cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)\n",
    "        img_input = resized_rgb\n",
    "        return img_input\n",
    "        \n",
    "    def run(self, image):\n",
    "        img_input = self.preprocess(image)\n",
    "\n",
    "        # Tensorflow V1에서는 model(input) 방식이 아니라 sess.run(feed_dict={input...}) 방식을 활용합니다.\n",
    "        batch_seg_map = self.sess.run(\n",
    "            self.OUTPUT_TENSOR_NAME,\n",
    "            feed_dict={self.INPUT_TENSOR_NAME: [img_input]})\n",
    "\n",
    "        seg_map = batch_seg_map[0]\n",
    "        return cv2.cvtColor(img_input, cv2.COLOR_RGB2BGR), seg_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-wesley",
   "metadata": {},
   "source": [
    "## 3. Define Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "active-mother",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp directory: /home/aiffel-dj53/aiffel/human_segmentation/models\n",
      "model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# define model and download & load pretrained weight\n",
    "_DOWNLOAD_URL_PREFIX = 'http://download.tensorflow.org/models/'\n",
    "\n",
    "model_dir = os.getenv('HOME')+'/aiffel/human_segmentation/models'\n",
    "tf.io.gfile.makedirs(model_dir)\n",
    "\n",
    "print ('temp directory:', model_dir)\n",
    "\n",
    "download_path = os.path.join(model_dir, 'deeplab_model.tar.gz')\n",
    "if not os.path.exists(download_path):\n",
    "    urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + 'deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz',\n",
    "                   download_path)\n",
    "\n",
    "MODEL = DeepLabModel(download_path)\n",
    "print('model loaded successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-corps",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_resized, seg_map = MODEL.run(img_orig)\n",
    "print (img_orig.shape, img_resized.shape, seg_map.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-lobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_NAMES = [\n",
    "    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    "    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tv'\n",
    "]\n",
    "len(LABEL_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-couple",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_show = img_resized.copy()\n",
    "seg_map = np.where(seg_map == 15, 15, 0) # 예측 중 사람만 추출\n",
    "img_mask = seg_map * (255/seg_map.max()) # 255 normalization\n",
    "img_mask = img_mask.astype(np.uint8)\n",
    "color_mask = cv2.applyColorMap(img_mask, cv2.COLORMAP_JET)\n",
    "img_show = cv2.addWeighted(img_show, 0.6, color_mask, 0.35, 0.0)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(img_show, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-duplicate",
   "metadata": {},
   "source": [
    "## 4. 결과 원래 크기로 복원하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mask_up = cv2.resize(img_mask, img_orig.shape[:2][::-1], interpolation=cv2.INTER_LINEAR)\n",
    "_, img_mask_up = cv2.threshold(img_mask_up, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "ax = plt.subplot(1,2,1)\n",
    "plt.imshow(img_mask_up, cmap=plt.cm.binary_r)\n",
    "ax.set_title('Original Size Mask')\n",
    "\n",
    "ax = plt.subplot(1,2,2)\n",
    "plt.imshow(img_mask, cmap=plt.cm.binary_r)\n",
    "ax.set_title('DeepLab Model Mask')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-parks",
   "metadata": {},
   "source": [
    "## 5. 배경 흐리게 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-rugby",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mask_color = cv2.cvtColor(img_mask_up, cv2.COLOR_GRAY2BGR)\n",
    "img_bg_mask = cv2.bitwise_not(img_mask_color)\n",
    "img_bg = cv2.bitwise_and(img_orig, img_bg_mask)\n",
    "plt.imshow(img_bg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-blast",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_bg_blur = cv2.blur(img_bg, (13,13))\n",
    "plt.imshow(cv2.cvtColor(img_bg_blur, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-association",
   "metadata": {},
   "source": [
    "## 6. 원본과 합성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-python",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img_concat = np.where(img_mask_color==255, img_orig, img_bg_blur)\n",
    "plt.imshow(cv2.cvtColor(img_concat, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-exchange",
   "metadata": {},
   "source": [
    "## 7. 문제점 찾아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-morgan",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv2.imwrite('img_concat1.jpg', img_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-orientation",
   "metadata": {},
   "source": [
    "![my_cropped](https://user-images.githubusercontent.com/74899925/108627799-baa39380-749a-11eb-8540-c8ebc43aa478.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-volume",
   "metadata": {},
   "source": [
    "### ㄴ 어깨와 팔 부근에서 약간의 격차가 있으며 주위로 검은색 실선이 나타난다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-wichita",
   "metadata": {},
   "source": [
    "# (2) # my_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "img_path = os.getenv('HOME')+'/aiffel/human_segmentation/images/my_cat.jpeg'  # 본인이 선택한 이미지의 경로에 맞게 바꿔 주세요. \n",
    "img_orig = cv2.imread(img_path) \n",
    "print (img_orig.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-banks",
   "metadata": {},
   "source": [
    "## 2. DeepLab model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabModel(object):\n",
    "    INPUT_TENSOR_NAME = 'ImageTensor:0'\n",
    "    OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'\n",
    "    INPUT_SIZE = 513\n",
    "    FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
    "\n",
    "    # __init__()에서 모델 구조를 직접 구현하는 대신, tar file에서 읽어들인 그래프구조 graph_def를 \n",
    "    # tf.compat.v1.import_graph_def를 통해 불러들여 활용하게 됩니다. \n",
    "    def __init__(self, tarball_path):\n",
    "        self.graph = tf.Graph()\n",
    "        graph_def = None\n",
    "        tar_file = tarfile.open(tarball_path)\n",
    "        for tar_info in tar_file.getmembers():\n",
    "            if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n",
    "                file_handle = tar_file.extractfile(tar_info)\n",
    "                graph_def = tf.compat.v1.GraphDef.FromString(file_handle.read())\n",
    "                break\n",
    "        tar_file.close()\n",
    "\n",
    "        with self.graph.as_default():\n",
    "            tf.compat.v1.import_graph_def(graph_def, name='')\n",
    "\n",
    "        self.sess = tf.compat.v1.Session(graph=self.graph)\n",
    "\n",
    "    # 이미지를 전처리하여 Tensorflow 입력으로 사용 가능한 shape의 Numpy Array로 변환합니다.\n",
    "    def preprocess(self, img_orig):\n",
    "        height, width = img_orig.shape[:2]\n",
    "        resize_ratio = 1.0 * self.INPUT_SIZE / max(width, height)\n",
    "        target_size = (int(resize_ratio * width), int(resize_ratio * height))\n",
    "        resized_image = cv2.resize(img_orig, target_size)\n",
    "        resized_rgb = cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)\n",
    "        img_input = resized_rgb\n",
    "        return img_input\n",
    "        \n",
    "    def run(self, image):\n",
    "        img_input = self.preprocess(image)\n",
    "\n",
    "        # Tensorflow V1에서는 model(input) 방식이 아니라 sess.run(feed_dict={input...}) 방식을 활용합니다.\n",
    "        batch_seg_map = self.sess.run(\n",
    "            self.OUTPUT_TENSOR_NAME,\n",
    "            feed_dict={self.INPUT_TENSOR_NAME: [img_input]})\n",
    "\n",
    "        seg_map = batch_seg_map[0]\n",
    "        return cv2.cvtColor(img_input, cv2.COLOR_RGB2BGR), seg_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-association",
   "metadata": {},
   "source": [
    "## 3. Define Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-worker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model and download & load pretrained weight\n",
    "_DOWNLOAD_URL_PREFIX = 'http://download.tensorflow.org/models/'\n",
    "\n",
    "model_dir = os.getenv('HOME')+'/aiffel/human_segmentation/models'\n",
    "tf.io.gfile.makedirs(model_dir)\n",
    "\n",
    "print ('temp directory:', model_dir)\n",
    "\n",
    "download_path = os.path.join(model_dir, 'deeplab_model.tar.gz')\n",
    "if not os.path.exists(download_path):\n",
    "    urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + 'deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz',\n",
    "                   download_path)\n",
    "\n",
    "MODEL = DeepLabModel(download_path)\n",
    "print('model loaded successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_resized, seg_map = MODEL.run(img_orig)\n",
    "print (img_orig.shape, img_resized.shape, seg_map.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-indiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_NAMES = [\n",
    "    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    "    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tv'\n",
    "]\n",
    "len(LABEL_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_show = img_resized.copy()\n",
    "seg_map = np.where(seg_map == 8, 8, 0) # 예측 중 고양이만추출\n",
    "img_mask = seg_map * (255/seg_map.max()) # 255 normalization\n",
    "img_mask = img_mask.astype(np.uint8)\n",
    "color_mask = cv2.applyColorMap(img_mask, cv2.COLORMAP_JET)\n",
    "img_show = cv2.addWeighted(img_show, 0.6, color_mask, 0.35, 0.0)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(img_show, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-settle",
   "metadata": {},
   "source": [
    "## 4. 결과 원래 크기로 복원하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-knife",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mask_up = cv2.resize(img_mask, img_orig.shape[:2][::-1], interpolation=cv2.INTER_LINEAR)\n",
    "_, img_mask_up = cv2.threshold(img_mask_up, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "ax = plt.subplot(1,2,1)\n",
    "plt.imshow(img_mask_up, cmap=plt.cm.binary_r)\n",
    "ax.set_title('Original Size Mask')\n",
    "\n",
    "ax = plt.subplot(1,2,2)\n",
    "plt.imshow(img_mask, cmap=plt.cm.binary_r)\n",
    "ax.set_title('DeepLab Model Mask')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-surgeon",
   "metadata": {},
   "source": [
    "## 5. 배경 흐리게 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-dependence",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mask_color = cv2.cvtColor(img_mask_up, cv2.COLOR_GRAY2BGR)\n",
    "img_bg_mask = cv2.bitwise_not(img_mask_color)\n",
    "img_bg = cv2.bitwise_and(img_orig, img_bg_mask)\n",
    "plt.imshow(img_bg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-climate",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_bg_blur = cv2.blur(img_bg, (13,13))\n",
    "plt.imshow(cv2.cvtColor(img_bg_blur, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-produce",
   "metadata": {},
   "source": [
    "## 6. 원본과 합성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-makeup",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_concat = np.where(img_mask_color==255, img_orig, img_bg_blur)\n",
    "plt.imshow(cv2.cvtColor(img_concat, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-liberty",
   "metadata": {},
   "source": [
    "## 7. 문제점 찾아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('img_concat2.jpg', img_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-membrane",
   "metadata": {},
   "source": [
    "![cat_cropped](https://user-images.githubusercontent.com/74899925/108628042-ed9a5700-749b-11eb-8f4a-de8222d848ab.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-prediction",
   "metadata": {},
   "source": [
    "### ㄴ 거품까지 인식되었으며 주위로 검은색 실선이 나타났다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-sigma",
   "metadata": {},
   "source": [
    "# (3) Chroma Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-rapid",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "img_path = os.getenv('HOME')+'/aiffel/human_segmentation/images/my_image.jpg'  # 본인이 선택한 이미지의 경로에 맞게 바꿔 주세요. \n",
    "img_orig = cv2.imread(img_path) \n",
    "print (img_orig.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-incentive",
   "metadata": {},
   "source": [
    "## 2. DeepLab model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-palestine",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabModel(object):\n",
    "    INPUT_TENSOR_NAME = 'ImageTensor:0'\n",
    "    OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'\n",
    "    INPUT_SIZE = 513\n",
    "    FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
    "\n",
    "    # __init__()에서 모델 구조를 직접 구현하는 대신, tar file에서 읽어들인 그래프구조 graph_def를 \n",
    "    # tf.compat.v1.import_graph_def를 통해 불러들여 활용하게 됩니다. \n",
    "    def __init__(self, tarball_path):\n",
    "        self.graph = tf.Graph()\n",
    "        graph_def = None\n",
    "        tar_file = tarfile.open(tarball_path)\n",
    "        for tar_info in tar_file.getmembers():\n",
    "            if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n",
    "                file_handle = tar_file.extractfile(tar_info)\n",
    "                graph_def = tf.compat.v1.GraphDef.FromString(file_handle.read())\n",
    "                break\n",
    "        tar_file.close()\n",
    "\n",
    "        with self.graph.as_default():\n",
    "            tf.compat.v1.import_graph_def(graph_def, name='')\n",
    "\n",
    "        self.sess = tf.compat.v1.Session(graph=self.graph)\n",
    "\n",
    "    # 이미지를 전처리하여 Tensorflow 입력으로 사용 가능한 shape의 Numpy Array로 변환합니다.\n",
    "    def preprocess(self, img_orig):\n",
    "        height, width = img_orig.shape[:2]\n",
    "        resize_ratio = 1.0 * self.INPUT_SIZE / max(width, height)\n",
    "        target_size = (int(resize_ratio * width), int(resize_ratio * height))\n",
    "        resized_image = cv2.resize(img_orig, target_size)\n",
    "        resized_rgb = cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)\n",
    "        img_input = resized_rgb\n",
    "        return img_input\n",
    "        \n",
    "    def run(self, image):\n",
    "        img_input = self.preprocess(image)\n",
    "\n",
    "        # Tensorflow V1에서는 model(input) 방식이 아니라 sess.run(feed_dict={input...}) 방식을 활용합니다.\n",
    "        batch_seg_map = self.sess.run(\n",
    "            self.OUTPUT_TENSOR_NAME,\n",
    "            feed_dict={self.INPUT_TENSOR_NAME: [img_input]})\n",
    "\n",
    "        seg_map = batch_seg_map[0]\n",
    "        return cv2.cvtColor(img_input, cv2.COLOR_RGB2BGR), seg_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-absence",
   "metadata": {},
   "source": [
    "## 3. Define Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model and download & load pretrained weight\n",
    "_DOWNLOAD_URL_PREFIX = 'http://download.tensorflow.org/models/'\n",
    "\n",
    "model_dir = os.getenv('HOME')+'/aiffel/human_segmentation/models'\n",
    "tf.io.gfile.makedirs(model_dir)\n",
    "\n",
    "print ('temp directory:', model_dir)\n",
    "\n",
    "download_path = os.path.join(model_dir, 'deeplab_model.tar.gz')\n",
    "if not os.path.exists(download_path):\n",
    "    urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + 'deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz',\n",
    "                   download_path)\n",
    "\n",
    "MODEL = DeepLabModel(download_path)\n",
    "print('model loaded successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_resized, seg_map = MODEL.run(img_orig)\n",
    "print (img_orig.shape, img_resized.shape, seg_map.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_NAMES = [\n",
    "    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    "    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tv'\n",
    "]\n",
    "len(LABEL_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-timer",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_show = img_resized.copy()\n",
    "seg_map = np.where(seg_map == 15, 15, 0) # 예측 중 사람만 추출\n",
    "img_mask = seg_map * (255/seg_map.max()) # 255 normalization\n",
    "img_mask = img_mask.astype(np.uint8)\n",
    "color_mask = cv2.applyColorMap(img_mask, cv2.COLORMAP_JET)\n",
    "img_show = cv2.addWeighted(img_show, 0.6, color_mask, 0.35, 0.0)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(img_show, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-loading",
   "metadata": {},
   "source": [
    "## 4. 결과 원래 크기로 복원하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-feelings",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mask_up = cv2.resize(img_mask, img_orig.shape[:2][::-1], interpolation=cv2.INTER_LINEAR)\n",
    "_, img_mask_up = cv2.threshold(img_mask_up, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "ax = plt.subplot(1,2,1)\n",
    "plt.imshow(img_mask_up, cmap=plt.cm.binary_r)\n",
    "ax.set_title('Original Size Mask')\n",
    "\n",
    "ax = plt.subplot(1,2,2)\n",
    "plt.imshow(img_mask, cmap=plt.cm.binary_r)\n",
    "ax.set_title('DeepLab Model Mask')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-outside",
   "metadata": {},
   "source": [
    "## 5. 배경과 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-disposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mask_color = cv2.cvtColor(img_mask_up, cv2.COLOR_GRAY2BGR)\n",
    "img_bg_mask = cv2.bitwise_not(img_mask_color)\n",
    "img_bg = cv2.bitwise_and(img_orig, img_bg_mask)\n",
    "plt.imshow(img_bg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-revolution",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(img_bg_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_bg_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-colombia",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_bg_blur = cv2.blur(img_bg, (13,13)) ## 블러 처리 함수\n",
    "plt.imshow(cv2.cvtColor(img_bg_blur, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-dylan",
   "metadata": {},
   "source": [
    "## 6. 크로마키 배경 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_path = os.getenv('HOME')+'/aiffel/human_segmentation/images/lake_moraine.jpg'  # 본인이 선택한 이미지의 경로에 맞게 바꿔 주세요. \n",
    "cm_orig = cv2.imread(cm_path) \n",
    "print (cm_orig.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-berkeley",
   "metadata": {},
   "source": [
    "### ㄴ 배경으로 쓸 이미지를 불러왔다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-project",
   "metadata": {},
   "source": [
    "## 크로마키를 적용할 방안을 궁리하다가 exp3 스티커사진 붙이기에서 공부했던 np.where()함수를 이용해 적용해보기로 했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cm_resize = cv2.resize(cm_orig, dsize=(4032, 3024))\n",
    "plt.imshow(cm_resize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-pepper",
   "metadata": {},
   "source": [
    "### ㄴ 적용할 배경을 resize해준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_img = np.where(img_bg_mask==0,img_bg_mask, cm_resize).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-mistake",
   "metadata": {},
   "source": [
    "### ㄴ np.where 함수를 사용해서 segmentation으로 분리했던 이미지와 결합해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-young",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(combined_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-association",
   "metadata": {},
   "source": [
    "### ㄴ 성공적으로 결합되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-watch",
   "metadata": {},
   "source": [
    "## 7. 크로마키 배경 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_concat = np.where(img_mask_color==255, img_orig, combined_img)\n",
    "plt.imshow(cv2.cvtColor(img_concat, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-waters",
   "metadata": {},
   "source": [
    "### ㄴ 짠! 완성되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-exhibition",
   "metadata": {},
   "source": [
    "# (4) 해결책 제시하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-brisbane",
   "metadata": {},
   "source": [
    "## DeepLab model은 배경과 인물의 경계선 (edge)를 기준으로 segmentation을 진행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-parish",
   "metadata": {},
   "source": [
    "![images](https://user-images.githubusercontent.com/74899925/108807277-50e4d000-75e7-11eb-86d8-85b0672f0455.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-syntax",
   "metadata": {},
   "source": [
    "## 그렇기때문에 배경과 사물 사이의 경계선이 segmentaion의 정확도에 크게 기여하는것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-stylus",
   "metadata": {},
   "source": [
    "## 위의 원리를 토대로 이미지에서 depth map 을 추출하여 윤곽선을 더 구체적으로 만들어주면 보다 정확도가 높은 segmentation이 도출될것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-wallace",
   "metadata": {},
   "source": [
    "![depth](https://user-images.githubusercontent.com/74899925/108807408-b1740d00-75e7-11eb-9a08-2068c32b7668.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-repository",
   "metadata": {},
   "source": [
    "### ㄴ depth estimator 로 추출된 depth map. 거리감에 따라 사물과 배경의 구분이 자세하게 구현되어있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-waters",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-stand",
   "metadata": {},
   "source": [
    "## 1. 카메라 어플에서 지원하는 크로마키 서비스가 어떤 방식으로 작동하게 되는지 알게된 프로젝트였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-cocktail",
   "metadata": {},
   "source": [
    "## 2. DeepLab model을 통해 구현을 해봤는데 사람뿐만이 아니라 고양이,자전거,차 등 여러가지 사물을 인식할수있는 기능이 탑재되어있다는게 놀라웠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-escape",
   "metadata": {},
   "source": [
    "## 3. NumPy의 유용함을 다시한번 실감했다. exp3에서 배웠던 np.where()함수를 다시 사용해 크로마키를 구현했는데 성공적으로 구현되어 뿌듯했다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-sudan",
   "metadata": {},
   "source": [
    "## 4. Depth Map과 비슷하게 원본 이미지를 Thermal mode로 전환하여 진행을 해봤었는데 결과가 더 안좋았다. Depth Map을 구현해보려면 더 많은 공부가 필요할것같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-jones",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
