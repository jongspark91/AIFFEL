{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "genuine-editing",
   "metadata": {},
   "source": [
    "# 1. 필요 모듈 및 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "scenic-performance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['When somebody loved me', 'Everything was beautiful', 'Every hour we spent together', 'Lives within my heart And when she was sad', 'I was there to dry her tears', 'And when she was happy so was I', 'When she loved me Through the summer and the fall', 'We had each other that was all', 'Just she and I together', 'Like it was meant to be And when she was lonely', 'I was there to comfort her', 'And I knew that she loved me So the years went by', 'I stayed the same', 'She began to drift away', 'I was left alone Still I waited for the day', \"When she'd say\", 'I will always love you Lonely and forgotten', \"Never thought she'd look my way\", 'She smiled at me and held me Just like she used to do', 'Like she loved me', 'When she loved me When somebody loved me', 'Everything was beautiful', 'Every hour we spent together', 'Lives within my heart When she loved me, hey Sentimentos são', 'Fáceis de mudar', 'Mesmo entre quem', 'Não vê que alguém', 'Pode ser seu par Basta um olhar', 'Que o outro não espera', 'Para assustar e até perturbar']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-offer",
   "metadata": {},
   "source": [
    "# 2. 정제 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "complimentary-startup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bulgarian-catalog",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> when she d say <end>',\n",
       " '<start> f ceis de mudar <end>',\n",
       " '<start> quando ele vem <end>',\n",
       " '<start> nada o det m <end>',\n",
       " '<start> para nos trazer <end>',\n",
       " '<start> novas sensa es <end>',\n",
       " '<start> doces emo es <end>',\n",
       " '<start> sentimentos s o <end>',\n",
       " '<start> como uma can o <end>',\n",
       " '<start> como uma can o <end>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if len(sentence) > 15: continue # 문장길이 15 이상인 가사들은 제외\n",
    "    if sentence[-1] == \":\": continue\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "front-greek",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  24.541784702549574\n",
      "문장길이 최대 :  35\n",
      "문장길이 표준편차 :  3.682949641428149\n"
     ]
    }
   ],
   "source": [
    "total_corpus = list(corpus)\n",
    "num_tokens = [len(tokens) for tokens in total_corpus]\n",
    "num_tokens = np.array(num_tokens)\n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-parking",
   "metadata": {},
   "source": [
    "### exp4에서 배운 내용을 사용하여 문장 길이 정보를 열람한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-soundtrack",
   "metadata": {},
   "source": [
    "# 3. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "incorrect-standing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   95   56 ...    0    0    0]\n",
      " [   2  677 1908 ...    0    0    0]\n",
      " [   2 1910 1911 ...    0    0    0]\n",
      " ...\n",
      " [   2  153   87 ...    0    0    0]\n",
      " [   2    9  417 ...    0    0    0]\n",
      " [   2 1703    3 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fbc351b8810>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "handy-flower",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2,  95,  56, 115,  52,   3,   0,   0,   0,   0,   0,   0,   0],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "elementary-petersburg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   95   56  115   52    3    0    0    0    0]\n",
      " [   2  677 1908  881 1909    3    0    0    0    0]\n",
      " [   2 1910 1911 1912    3    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "engaging-helmet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : .\n",
      "7 : you\n",
      "8 : oh\n",
      "9 : it\n",
      "10 : me\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "swedish-marketing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  95  56 115  52   3   0   0   0   0   0   0]\n",
      "[ 95  56 115  52   3   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "changed-pottery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 12), (256, 12)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-creator",
   "metadata": {},
   "source": [
    "# 4. 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "supported-boating",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "executive-bracelet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 12, 7001), dtype=float32, numpy=\n",
       "array([[[-2.70733377e-04, -2.56102649e-04,  7.46463920e-05, ...,\n",
       "         -2.83964096e-06,  1.93816726e-04, -3.40270490e-04],\n",
       "        [-5.30796533e-04, -1.48584746e-04, -1.72949531e-05, ...,\n",
       "          4.84152042e-05,  2.79593369e-04, -4.37705021e-04],\n",
       "        [-6.31079602e-04,  1.24627273e-04,  1.01788974e-04, ...,\n",
       "         -5.92460674e-05,  5.17813489e-04, -1.27993218e-04],\n",
       "        ...,\n",
       "        [-1.77717593e-03,  2.51413498e-04,  6.39257370e-04, ...,\n",
       "          1.72288029e-03,  1.48803042e-03, -2.94703405e-05],\n",
       "        [-2.06236588e-03,  2.43604940e-04,  6.16219535e-04, ...,\n",
       "          1.90811243e-03,  1.72253628e-03, -6.18208869e-05],\n",
       "        [-2.33219331e-03,  2.26869277e-04,  5.81969041e-04, ...,\n",
       "          2.03832984e-03,  1.95649359e-03, -9.89169057e-05]],\n",
       "\n",
       "       [[-2.70733377e-04, -2.56102649e-04,  7.46463920e-05, ...,\n",
       "         -2.83964096e-06,  1.93816726e-04, -3.40270490e-04],\n",
       "        [-5.66275383e-04, -6.03241380e-04,  3.76718264e-04, ...,\n",
       "          3.29585950e-04,  1.03800430e-05, -1.66177473e-04],\n",
       "        [-6.75166782e-04, -6.13589014e-04,  3.99150595e-04, ...,\n",
       "          6.52248331e-04, -5.85197085e-05,  7.03966653e-05],\n",
       "        ...,\n",
       "        [-1.12018548e-03, -4.16758005e-04,  4.81532828e-04, ...,\n",
       "          1.58419891e-03,  8.98024999e-04,  4.48143343e-04],\n",
       "        [-1.52191636e-03, -3.30877141e-04,  4.53103043e-04, ...,\n",
       "          1.80438231e-03,  1.23064220e-03,  3.74270254e-04],\n",
       "        [-1.91111665e-03, -2.65222799e-04,  4.10015287e-04, ...,\n",
       "          1.96133275e-03,  1.56573951e-03,  2.87825213e-04]],\n",
       "\n",
       "       [[-2.70733377e-04, -2.56102649e-04,  7.46463920e-05, ...,\n",
       "         -2.83964096e-06,  1.93816726e-04, -3.40270490e-04],\n",
       "        [-6.53134601e-04, -4.88937018e-04,  3.18096514e-04, ...,\n",
       "          7.81917770e-05,  6.54687872e-04, -2.78061605e-04],\n",
       "        [-1.01781753e-03, -3.22120555e-04,  5.67855081e-04, ...,\n",
       "          2.23461597e-04,  7.42489588e-04,  1.53318775e-04],\n",
       "        ...,\n",
       "        [-1.32897205e-03, -3.75830830e-04,  1.73712921e-04, ...,\n",
       "          1.85863930e-03,  2.51438905e-04,  3.55334923e-04],\n",
       "        [-1.60450290e-03, -3.01372580e-04,  1.39983968e-04, ...,\n",
       "          2.03939667e-03,  5.53943624e-04,  3.29898729e-04],\n",
       "        [-1.89290941e-03, -2.34410400e-04,  1.03436112e-04, ...,\n",
       "          2.16152542e-03,  8.97455495e-04,  2.81574088e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-2.70733377e-04, -2.56102649e-04,  7.46463920e-05, ...,\n",
       "         -2.83964096e-06,  1.93816726e-04, -3.40270490e-04],\n",
       "        [-5.66275383e-04, -6.03241380e-04,  3.76718264e-04, ...,\n",
       "          3.29585950e-04,  1.03800430e-05, -1.66177473e-04],\n",
       "        [-8.37864121e-04, -7.92055565e-04,  5.26266813e-04, ...,\n",
       "          2.56765925e-04, -8.52424346e-05,  1.50499021e-04],\n",
       "        ...,\n",
       "        [-1.75779907e-03, -7.97239423e-04,  4.30423970e-04, ...,\n",
       "          1.41892757e-03,  1.22932100e-03,  3.59831465e-05],\n",
       "        [-2.05187406e-03, -7.12232257e-04,  3.97543685e-04, ...,\n",
       "          1.58225116e-03,  1.56460528e-03, -2.35974239e-05],\n",
       "        [-2.33380101e-03, -6.36334647e-04,  3.65819607e-04, ...,\n",
       "          1.70840544e-03,  1.88390107e-03, -8.23033115e-05]],\n",
       "\n",
       "       [[-2.70733377e-04, -2.56102649e-04,  7.46463920e-05, ...,\n",
       "         -2.83964096e-06,  1.93816726e-04, -3.40270490e-04],\n",
       "        [-4.73536318e-04, -1.42751378e-04,  3.27706657e-04, ...,\n",
       "         -2.18335204e-04,  4.86925215e-04, -4.13850852e-04],\n",
       "        [-4.54645517e-04, -2.87373317e-04,  5.58368978e-04, ...,\n",
       "         -3.75463977e-04,  7.66112527e-04, -8.05901771e-04],\n",
       "        ...,\n",
       "        [-1.98238785e-03,  9.30185197e-05,  4.58656286e-04, ...,\n",
       "          1.65472669e-03,  1.51148392e-03, -3.23112938e-04],\n",
       "        [-2.25907890e-03,  1.14291623e-04,  3.96979362e-04, ...,\n",
       "          1.81159948e-03,  1.75295339e-03, -3.20697494e-04],\n",
       "        [-2.51314673e-03,  1.23754187e-04,  3.35879042e-04, ...,\n",
       "          1.92191743e-03,  1.98696274e-03, -3.21031519e-04]],\n",
       "\n",
       "       [[-2.70733377e-04, -2.56102649e-04,  7.46463920e-05, ...,\n",
       "         -2.83964096e-06,  1.93816726e-04, -3.40270490e-04],\n",
       "        [-2.85818649e-04, -2.16099404e-04, -2.33066257e-05, ...,\n",
       "          1.74147703e-04,  2.27424593e-04, -5.01126109e-04],\n",
       "        [-1.56224123e-04, -9.76654846e-05, -3.05199559e-04, ...,\n",
       "          1.05260704e-04,  1.44494392e-04, -5.53995196e-04],\n",
       "        ...,\n",
       "        [-1.56023249e-03, -4.92720748e-04,  3.18572100e-04, ...,\n",
       "          1.68251863e-03,  6.99501426e-04, -4.08855209e-04],\n",
       "        [-1.89954508e-03, -4.52223438e-04,  3.14604142e-04, ...,\n",
       "          1.87564071e-03,  1.01578212e-03, -3.92689195e-04],\n",
       "        [-2.21391465e-03, -4.12053516e-04,  2.93318240e-04, ...,\n",
       "          2.01822794e-03,  1.33581448e-03, -3.87719949e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dated-vulnerability",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1792256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  7176025   \n",
      "=================================================================\n",
      "Total params: 22,607,961\n",
      "Trainable params: 22,607,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-romania",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "duplicate-bunch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "55/55 [==============================] - 5s 97ms/step - loss: 2.6715\n",
      "Epoch 2/30\n",
      "55/55 [==============================] - 5s 88ms/step - loss: 1.5737\n",
      "Epoch 3/30\n",
      "55/55 [==============================] - 5s 87ms/step - loss: 1.4303\n",
      "Epoch 4/30\n",
      "55/55 [==============================] - 5s 88ms/step - loss: 1.3842\n",
      "Epoch 5/30\n",
      "55/55 [==============================] - 5s 88ms/step - loss: 1.3571\n",
      "Epoch 6/30\n",
      "55/55 [==============================] - 5s 88ms/step - loss: 1.3330\n",
      "Epoch 7/30\n",
      "55/55 [==============================] - 5s 88ms/step - loss: 1.3069\n",
      "Epoch 8/30\n",
      "55/55 [==============================] - 5s 88ms/step - loss: 1.2766\n",
      "Epoch 9/30\n",
      "55/55 [==============================] - 5s 89ms/step - loss: 1.2438\n",
      "Epoch 10/30\n",
      "55/55 [==============================] - 5s 89ms/step - loss: 1.2142\n",
      "Epoch 11/30\n",
      "55/55 [==============================] - 5s 89ms/step - loss: 1.1889\n",
      "Epoch 12/30\n",
      "55/55 [==============================] - 5s 89ms/step - loss: 1.1619\n",
      "Epoch 13/30\n",
      "55/55 [==============================] - 5s 89ms/step - loss: 1.1358\n",
      "Epoch 14/30\n",
      "55/55 [==============================] - 5s 89ms/step - loss: 1.1125\n",
      "Epoch 15/30\n",
      "55/55 [==============================] - 5s 89ms/step - loss: 1.0899\n",
      "Epoch 16/30\n",
      "55/55 [==============================] - 5s 89ms/step - loss: 1.0686\n",
      "Epoch 17/30\n",
      "55/55 [==============================] - 5s 90ms/step - loss: 1.0488\n",
      "Epoch 18/30\n",
      "55/55 [==============================] - 5s 89ms/step - loss: 1.0300\n",
      "Epoch 19/30\n",
      "55/55 [==============================] - 5s 89ms/step - loss: 1.0122\n",
      "Epoch 20/30\n",
      "55/55 [==============================] - 5s 90ms/step - loss: 0.9949\n",
      "Epoch 21/30\n",
      "55/55 [==============================] - 5s 92ms/step - loss: 0.9813\n",
      "Epoch 22/30\n",
      "55/55 [==============================] - 5s 91ms/step - loss: 0.9668\n",
      "Epoch 23/30\n",
      "55/55 [==============================] - 5s 94ms/step - loss: 0.9524\n",
      "Epoch 24/30\n",
      "55/55 [==============================] - 5s 96ms/step - loss: 0.9395\n",
      "Epoch 25/30\n",
      "55/55 [==============================] - 5s 96ms/step - loss: 0.9274\n",
      "Epoch 26/30\n",
      "55/55 [==============================] - 5s 91ms/step - loss: 0.9160\n",
      "Epoch 27/30\n",
      "55/55 [==============================] - 5s 96ms/step - loss: 0.9044\n",
      "Epoch 28/30\n",
      "55/55 [==============================] - 5s 93ms/step - loss: 0.8947\n",
      "Epoch 29/30\n",
      "55/55 [==============================] - 5s 92ms/step - loss: 0.8844\n",
      "Epoch 30/30\n",
      "55/55 [==============================] - 5s 93ms/step - loss: 0.8753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbc34ffe590>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-integrity",
   "metadata": {},
   "source": [
    "# 6. 작사가 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "satisfactory-salad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "attempted-looking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you re the one <end> '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-mobility",
   "metadata": {},
   "source": [
    "## 작사기가 잘 구현되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-catch",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-abortion",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-asset",
   "metadata": {},
   "source": [
    "## 1. Exp4에서 공부했었던 자연어처리를 거의 이해못한채로 진행했었는데 이번에 답습하는 느낌이여서 좋았다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-professor",
   "metadata": {},
   "source": [
    "## 2. 저번시간에 Token에 대한 개념을 제대로 이해 못했었는데 이번 노드를 통해 어떤 기능을 하는 구체적으로 알게되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-crossing",
   "metadata": {},
   "source": [
    "# 3. 앞으로도 자연어 처리 노드를 통해 좀더 역량을 개발 할 수 있는 계기가 있었으면 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-detection",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
